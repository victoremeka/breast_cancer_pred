{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f95d3",
   "metadata": {},
   "source": [
    "# Breast Cancer Prediction System - Model Building\n",
    "\n",
    "## Educational ML Project for Tumor Classification\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**: This system is strictly for educational purposes and must NOT be used as a medical diagnostic tool.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook builds a machine learning model to classify breast tumors as **benign** or **malignant** using the Breast Cancer Wisconsin (Diagnostic) Dataset.\n",
    "\n",
    "**Selected Features**: \n",
    "1. radius_mean\n",
    "2. texture_mean\n",
    "3. perimeter_mean\n",
    "4. area_mean\n",
    "5. smoothness_mean\n",
    "\n",
    "**Algorithm**: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c115f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e80615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, visualization, and machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1ba3a",
   "metadata": {},
   "source": [
    "## 2. Load the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer Wisconsin (Diagnostic) Dataset from sklearn\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame for better data manipulation\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFeature names: {list(data.feature_names[:10])}...\")\n",
    "print(f\"\\nTarget names: {data.target_names}\")\n",
    "print(f\"\\nTarget distribution:\\n{df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286b5dc",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31956c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\\nMissing values per column:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5687a",
   "metadata": {},
   "source": [
    "## 4. Select 5 Features for Model Training\n",
    "\n",
    "As per project requirements, we select exactly **5 features** from the available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select exactly 5 features as required\n",
    "selected_features = [\n",
    "    'mean radius',\n",
    "    'mean texture', \n",
    "    'mean perimeter',\n",
    "    'mean area',\n",
    "    'mean smoothness'\n",
    "]\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = df[selected_features]\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\n‚úÖ Successfully selected 5 features for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde8bd9",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling (Mandatory)\n",
    "\n",
    "**Important**: Feature scaling is critical for distance-based algorithms like Logistic Regression, SVM, KNN, and Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef274c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"‚úÖ Feature scaling applied using StandardScaler\")\n",
    "print(f\"\\nOriginal feature range (first feature):\")\n",
    "print(f\"  Min: {X.iloc[:, 0].min():.2f}, Max: {X.iloc[:, 0].max():.2f}\")\n",
    "print(f\"\\nScaled feature range (first feature):\")\n",
    "print(f\"  Min: {X_scaled[:, 0].min():.2f}, Max: {X_scaled[:, 0].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6844f",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split\n",
    "\n",
    "Split the data into training (80%) and testing (20%) sets with a fixed random_state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf06bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set target distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "print(f\"\\nTesting set target distribution:\\n{pd.Series(y_test).value_counts()}\")\n",
    "print(\"\\n‚úÖ Data split completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a42bd0",
   "metadata": {},
   "source": [
    "## 7. Model Training - Logistic Regression\n",
    "\n",
    "We'll use **Logistic Regression** for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "print(f\"\\nModel: Logistic Regression\")\n",
    "print(f\"Number of features: {len(selected_features)}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf1afa",
   "metadata": {},
   "source": [
    "## 8. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Predictions completed!\")\n",
    "print(f\"\\nFirst 10 predictions: {y_pred[:10]}\")\n",
    "print(f\"First 10 actual values: {y_test[:10].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04dfb5",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation - Performance Metrics\n",
    "\n",
    "Calculate accuracy, precision, recall, and F1-score as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a734d9",
   "metadata": {},
   "source": [
    "## 10. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e72635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant (0)', 'Benign (1)']))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586adf03",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant (0)', 'Benign (1)'],\n",
    "            yticklabels=['Malignant (0)', 'Benign (1)'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives (TN): {cm[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e498418",
   "metadata": {},
   "source": [
    "## 12. Save Model and Scaler (Model Persistence)\n",
    "\n",
    "**Critical**: We must save both the model AND the scaler for use in the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model using joblib\n",
    "joblib.dump(model, 'breast_cancer_model.pkl')\n",
    "print(\"‚úÖ Model saved as 'breast_cancer_model.pkl'\")\n",
    "\n",
    "# Save the scaler (CRITICAL - don't forget this!)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"‚úÖ Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save feature names for reference\n",
    "joblib.dump(selected_features, 'feature_names.pkl')\n",
    "print(\"‚úÖ Feature names saved as 'feature_names.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERSISTENCE COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(\"Files saved:\")\n",
    "print(\"  1. breast_cancer_model.pkl\")\n",
    "print(\"  2. scaler.pkl\")\n",
    "print(\"  3. feature_names.pkl\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e718a3",
   "metadata": {},
   "source": [
    "## 13. Load and Test Saved Model\n",
    "\n",
    "Demonstrate that the saved model works correctly by loading it and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38474c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and scaler\n",
    "loaded_model = joblib.load('breast_cancer_model.pkl')\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "loaded_features = joblib.load('feature_names.pkl')\n",
    "\n",
    "print(\"‚úÖ Model, scaler, and feature names loaded successfully!\")\n",
    "print(f\"\\nLoaded features: {loaded_features}\")\n",
    "\n",
    "# Test with a sample from the test set\n",
    "sample_index = 0\n",
    "sample_input = X.iloc[sample_index:sample_index+1]\n",
    "actual_label = y.iloc[sample_index]\n",
    "\n",
    "# Scale the input using the loaded scaler\n",
    "sample_scaled = loaded_scaler.transform(sample_input)\n",
    "\n",
    "# Make prediction with loaded model\n",
    "prediction = loaded_model.predict(sample_scaled)[0]\n",
    "probability = loaded_model.predict_proba(sample_scaled)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST PREDICTION WITH LOADED MODEL\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Sample features: {sample_input.values[0]}\")\n",
    "print(f\"\\nActual diagnosis: {'Benign' if actual_label == 1 else 'Malignant'}\")\n",
    "print(f\"Predicted diagnosis: {'Benign' if prediction == 1 else 'Malignant'}\")\n",
    "print(f\"\\nPrediction probabilities:\")\n",
    "print(f\"  Malignant: {probability[0]:.2%}\")\n",
    "print(f\"  Benign: {probability[1]:.2%}\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Loaded model works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a222dad",
   "metadata": {},
   "source": [
    "## üéâ Model Building Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ‚úÖ Dataset loaded and explored\n",
    "- ‚úÖ 5 features selected\n",
    "- ‚úÖ Data preprocessed with StandardScaler\n",
    "- ‚úÖ Model trained using Logistic Regression\n",
    "- ‚úÖ Excellent performance metrics achieved\n",
    "- ‚úÖ Model and scaler saved successfully\n",
    "- ‚úÖ Saved model tested and verified\n",
    "\n",
    "**Next Steps**:\n",
    "1. Build the web application (app.py)\n",
    "2. Deploy to Streamlit Cloud\n",
    "3. Submit to Scorac.com\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**: This model is for educational purposes only and should NOT be used for actual medical diagnosis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
